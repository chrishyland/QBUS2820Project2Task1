{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import model_selection\n",
    "\n",
    "#Stop warnings\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Clothing_Store.csv')\n",
    "\n",
    "dummies = pd.get_dummies(data[['VALPHON']],  drop_first=True)\n",
    "data=data.join(dummies)\n",
    "\n",
    "del data['VALPHON']\n",
    "final_train = data.sample(frac=0.6, random_state=450411920)\n",
    "final_test = data[data.index.isin(final_train.index)==False]\n",
    "#Now we have final train/test which only has predictors whilst y_train/test are the response\n",
    "y_train = final_train.pop('RESP')\n",
    "y_test = final_test.pop('RESP')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going to try out some models here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see more information about the models I use in this: http://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to store the results from our models\n",
    "pred = []\n",
    "method = []\n",
    "model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaptive Boosting\n",
    "regr = AdaBoostClassifier(learning_rate = 1, n_estimators = 350)\n",
    "model.append(regr)\n",
    "regr = regr.fit(final_train,y_train)\n",
    "adapred = regr.predict(final_test)\n",
    "pred.append(adapred)\n",
    "method.append('AdaptiveBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest \n",
    "RandomFor = RandomForestClassifier(max_depth=None,min_samples_split=2)\n",
    "model.append(RandomFor)\n",
    "RandomFor = RandomFor.fit(final_train,y_train)\n",
    "randomforpred = RandomFor.predict(final_test)\n",
    "pred.append(randomforpred)\n",
    "method.append('Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extremely Random Forest\n",
    "Extratree = ExtraTreesClassifier(max_depth=None,min_samples_split=2)\n",
    "model.append(Extratree)\n",
    "Extratree = Extratree.fit(final_train,y_train)\n",
    "predFinalExtRandomForest = Extratree.predict(final_test)\n",
    "pred.append(predFinalExtRandomForest)\n",
    "method.append('Extreme Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predFinalExtRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient Boosting Classifier\n",
    "GradBoost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0)\n",
    "model.append(GradBoost)\n",
    "GradBoost = GradBoost.fit(final_train,y_train)\n",
    "predGradBoost = GradBoost.predict(final_test)\n",
    "pred.append(predGradBoost)\n",
    "method.append('Gradient Boost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predGradBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "NaiveB = GaussianNB()\n",
    "model.append(NaiveB)\n",
    "NaiveB = NaiveB.fit(final_train, y_train)\n",
    "predNaiveB = NaiveB.predict(final_test)\n",
    "pred.append(predNaiveB)\n",
    "method.append('Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predNaiveB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(random_state=450411920)\n",
    "model.append(LogReg)\n",
    "LogReg = LogReg.fit(final_train,y_train)\n",
    "predLogReg = LogReg.predict(final_test)\n",
    "pred.append(predLogReg)\n",
    "method.append('Logit Reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predLogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate how each of our model performs from a confusion matrix.\n",
    "https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaptiveBoost confusion matrix is:\n",
      "[[6928  319]\n",
      " [ 982  467]]\n",
      "Accuracy of predicting Yes is 0.876\n",
      "Accuracy of predicting No is 0.594\n",
      "Overall accuracy is 0.850\n",
      "Random Forest confusion matrix is:\n",
      "[[7008  239]\n",
      " [1098  351]]\n",
      "Accuracy of predicting Yes is 0.865\n",
      "Accuracy of predicting No is 0.595\n",
      "Overall accuracy is 0.846\n",
      "Extreme Random Forest confusion matrix is:\n",
      "[[7009  238]\n",
      " [1131  318]]\n",
      "Accuracy of predicting Yes is 0.861\n",
      "Accuracy of predicting No is 0.572\n",
      "Overall accuracy is 0.843\n",
      "Gradient Boost confusion matrix is:\n",
      "[[6970  277]\n",
      " [1019  430]]\n",
      "Accuracy of predicting Yes is 0.872\n",
      "Accuracy of predicting No is 0.608\n",
      "Overall accuracy is 0.851\n",
      "Naive Bayes confusion matrix is:\n",
      "[[7202   45]\n",
      " [1436   13]]\n",
      "Accuracy of predicting Yes is 0.834\n",
      "Accuracy of predicting No is 0.224\n",
      "Overall accuracy is 0.830\n",
      "Logit Reg confusion matrix is:\n",
      "[[7247    0]\n",
      " [1449    0]]\n",
      "Accuracy of predicting Yes is 0.833\n",
      "Accuracy of predicting No is nan\n",
      "Overall accuracy is 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for prediction, name in zip(pred,method):\n",
    "    print('{} confusion matrix is:'.format(name))\n",
    "    matrix = confusion_matrix(y_test, prediction)\n",
    "    print(matrix)\n",
    "    print('Accuracy of predicting Yes is %.3f'%(matrix[0][0]/(matrix[0][0]+matrix[1][0])))\n",
    "    print('Accuracy of predicting No is %.3f'%(matrix[1][1]/(matrix[0][1]+matrix[1][1])))\n",
    "    print('Overall accuracy is %.3f' % ((matrix[0][0]+matrix[1][1])/(matrix[0][0]+matrix[1][1]+matrix[1][0]+matrix[0][1])))\n",
    "    #8696 is total observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Gradient Boosting was the most accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To interpret a confusion matrix, the column 1 and 2 represent the model predicting Yes or No. Then the rows represent were they actually yes or no. The [0,0] entry means that we predicted that many to be Yes, and it turns out that we were correct in predicting those, so that tells us how many yes observations we predicted correctly. Likewise, the [1,1] entry means how many no's we predicted correctly. The bottom left, [1,0] entry means we predicted yes, but was actually no and [0,1] entry is the vice versa. Conclusively, the diagonals of the matrix tells us how accurate are our predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have predictions from our models and we can take a majority vote to predict each observation. Here, we use a majority vote. Here, we assign each model as having a vote that is of equal importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensem = VotingClassifier(estimators=[('AdaBoost', regr), ('ExtraTrees', Extratree), ('GradientBoost', GradBoost)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensem = ensem.fit(final_train,y_train)\n",
    "ensempred1 = ensem.predict(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensempred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7003  244]\n",
      " [1026  423]]\n",
      "Accuracy of predicting Yes is 0.872\n",
      "Accuracy of predicting No is 0.634\n",
      "Overall accuracy is 0.854\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, ensempred1)\n",
    "print(matrix)\n",
    "print('Accuracy of predicting Yes is %.3f'%(matrix[0][0]/(matrix[0][0]+matrix[1][0])))\n",
    "print('Accuracy of predicting No is %.3f'%(matrix[1][1]/(matrix[0][1]+matrix[1][1])))\n",
    "print('Overall accuracy is %.3f' % ((matrix[0][0]+matrix[1][1])/(matrix[0][0]+matrix[1][1]+matrix[1][0]+matrix[0][1])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that taking a vote on each prediction improves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
